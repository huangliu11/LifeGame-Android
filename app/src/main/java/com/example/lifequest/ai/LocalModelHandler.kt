package com.example.lifequest.ai

import android.content.Context
import android.util.Log
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.io.File

/**
 * LocalModelHandler - é«˜çº§æ¨¡å‹ç®¡ç†å™¨
 * ä½¿ç”¨ LlamaInference ä½œä¸ºåº•å±‚ï¼Œæä¾›æ›´é«˜çº§çš„åŠŸèƒ½
 */
class LocalModelHandler(private val context: Context) {

    companion object {
        private const val TAG = "LocalModelHandler"
        private const val DEFAULT_MAX_TOKENS = 512
        private const val DEFAULT_TEMPERATURE = 0.7f
    }

    // âœ… ä½¿ç”¨ LlamaInference ä½œä¸ºåº•å±‚
    private var llamaInference: LlamaInference? = null
    private var isInitialized = false
    private var modelPath: String? = null
    private val useMockMode = false // ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼Œæµ‹è¯•é˜¶æ®µå…ˆä¸å¼€å¯

    /**
     * åˆå§‹åŒ–æ¨¡å‹
     */
    suspend fun initialize(modelFilePath: String? = null): Boolean = withContext(Dispatchers.IO) {
        try {
            if (isInitialized) {
                Log.d(TAG, "Model already initialized")
                return@withContext true
            }

            val path = modelFilePath ?: getDefaultModelPath()

            if (path == null) {
                Log.e(TAG, "Model path is null")
                return@withContext false
            }

            val modelFile = File(path)
            if (!modelFile.exists()) {
                Log.e(TAG, "Model file not found: $path")
                return@withContext false
            }

            Log.d(TAG, "Initializing model from: $path")

            if (useMockMode) {
                Log.d(TAG, "Using mock mode")
                isInitialized = true
                modelPath = path
                return@withContext true
            }

            // âœ… ä½¿ç”¨ LlamaInference åˆå§‹åŒ–
            llamaInference = LlamaInference()
            val success = llamaInference?.initialize(path) ?: false

            if (success) {
                isInitialized = true
                modelPath = path
                Log.d(TAG, "Model initialized successfully")
                true
            } else {
                Log.e(TAG, "Failed to initialize model")
                llamaInference = null
                false
            }
        } catch (e: Exception) {
            Log.e(TAG, "Error initializing model", e)
            false
        }
    }

    /**
     * ç”Ÿæˆå›å¤
     */
    suspend fun generate(
        prompt: String,
        maxTokens: Int = 100,
        temperature: Float = DEFAULT_TEMPERATURE,
        systemPrompt: String = ""
    ): String = withContext(Dispatchers.IO) {
        try {
            Log.d(TAG, "=== LocalModelHandler.generate START ===")
            Log.d(TAG, "Mode: ${if (useMockMode) "MOCK" else "REAL MODEL"}")
            Log.d(TAG, "Max tokens: $maxTokens")
            Log.d(TAG, "Prompt length: ${prompt.length}")
            Log.d(TAG, "System prompt length: ${systemPrompt.length}")
            Log.d(TAG, "Start time: ${System.currentTimeMillis()}")

            if (!isInitialized) {
                Log.e(TAG, "Model not initialized")
                return@withContext ""
            }
            val startTime = System.currentTimeMillis()

//            Log.d(TAG, "Generating response for: $prompt")

            val fullPrompt = if (systemPrompt.isNotEmpty()) {
                """
                <|system|>
                $systemPrompt
                <|end|>
                <|user|>
                $prompt
                <|end|>
                <|assistant|>
                """.trimIndent()
            } else {
                prompt
            }

            Log.d(TAG, "Full prompt length: ${fullPrompt.length}")
            Log.d(TAG, "Full prompt preview: $fullPrompt")

            val response = if (useMockMode) {
                Log.d(TAG, "Using MOCK mode")
                generateMockResponse(prompt)
            } else {
                Log.d(TAG, "Using REAL MODEL")

                // æ„å»ºå®Œæ•´ prompt
                val fullPrompt = if (systemPrompt.isNotEmpty()) {
                    """
                    <|system|>
                    $systemPrompt
                    <|end|>
                    <|user|>
                    $prompt
                    <|end|>
                    <|assistant|>
                    """.trimIndent()
                } else {
                    prompt
                }

                Log.d(TAG, "Full prompt length: ${fullPrompt.length}")
                Log.d(TAG, "Full prompt preview: $fullPrompt")

                try {
                    Log.d(TAG, "Calling llamaInference.generate()...")
                    val inferenceStart = System.currentTimeMillis()

                    val result = llamaInference?.generate(fullPrompt, maxTokens)

                    val inferenceDuration = System.currentTimeMillis() - inferenceStart
                    Log.d(TAG, "Native inference took: ${inferenceDuration}ms")

                    if (result.isNullOrEmpty()) {
                        Log.w(TAG, "âš ï¸ Native inference returned empty, using mock")
                        generateMockResponse(prompt)
                    } else {
                        Log.d(TAG, "âœ… Native inference success")
                        result
                    }
                } catch (e: Exception) {
                    Log.e(TAG, "âŒ Native inference error", e)
                    Log.e(TAG, "Error type: ${e.javaClass.simpleName}")
                    Log.e(TAG, "Error message: ${e.message}")
                    generateMockResponse(prompt)
                }
            }

            val totalDuration = System.currentTimeMillis() - startTime

            Log.d(TAG, "Total generation time: ${totalDuration}ms (${totalDuration / 1000.0}s)")
            Log.d(TAG, "Response length: ${response.length}")
            Log.d(TAG, "Response preview: $response")
            Log.d(TAG, "Tokens per second: ${if (totalDuration > 0) maxTokens * 1000.0 / totalDuration else 0}")
            Log.d(TAG, "=== LocalModelHandler.generate END ===")

            // âœ… æ€§èƒ½åˆ†æ
            when {
                totalDuration < 1000 -> Log.d(TAG, "âœ… Performance: EXCELLENT")
                totalDuration < 5000 -> Log.d(TAG, "âœ… Performance: GOOD")
                totalDuration < 15000 -> Log.w(TAG, "âš ï¸ Performance: ACCEPTABLE")
                else -> Log.e(TAG, "âŒ Performance: POOR (${totalDuration / 1000}s)")
            }

            response
        } catch (e: Exception) {
            Log.e(TAG, "Error generating response", e)
            "æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯ï¼š${e.message}"
        }
    }

    /**
     * ç”Ÿæˆæ¨¡æ‹Ÿå›å¤
     */
    private fun generateMockResponse(prompt: String): String {
        val lowerPrompt = prompt.lowercase()
        return when {
            lowerPrompt.contains("å¸®åŠ©") || lowerPrompt.contains("æ€ä¹ˆç”¨") -> {
                """
                æˆ‘å¯ä»¥å¸®ä½ ï¼š
                âœ¨ åˆ›å»ºä»»åŠ¡ - å‘Šè¯‰æˆ‘ä½ æƒ³åšä»€ä¹ˆï¼Œæˆ‘ä¼šå¸®ä½ åˆ›å»ºåˆé€‚çš„ä»»åŠ¡ã€‚
                ğŸ“‹ ä»»åŠ¡å»ºè®® - æˆ‘å¯ä»¥æ ¹æ®ä½ çš„æƒ…å†µæä¾›ä»»åŠ¡è§„åˆ’å»ºè®®ã€‚
                ğŸ’ª é¼“åŠ±æ”¯æŒ - åœ¨ä½ éœ€è¦çš„æ—¶å€™ç»™äºˆé¼“åŠ±å’Œæ”¯æŒã€‚
                
                è¯•è¯•å¯¹æˆ‘è¯´ï¼š
                â€¢ "åˆ›å»ºä¸»çº¿ä»»åŠ¡ï¼šå®Œæˆé¡¹ç›®æŠ¥å‘Š"
                â€¢ "å¸®æˆ‘åˆ¶å®šå­¦ä¹ è®¡åˆ’"
                â€¢ "æˆ‘æƒ³å…»æˆæ—©èµ·çš„ä¹ æƒ¯"
                """.trimIndent()
            }
            lowerPrompt.contains("è®¡åˆ’") || lowerPrompt.contains("è§„åˆ’") -> {
                """
                åˆ¶å®šè®¡åˆ’æ˜¯ä¸ªå¥½ä¸»æ„ï¼
                å»ºè®®ä½ ï¼š
                1. å…ˆç¡®å®šä¸»è¦ç›®æ ‡
                2. åˆ†è§£æˆå°ä»»åŠ¡
                3. è®¾å®šåˆç†çš„æ—¶é—´
                4. æ¯å¤©å®Œæˆä¸€ç‚¹
                
                å‘Šè¯‰æˆ‘ä½ çš„å…·ä½“ç›®æ ‡ï¼Œæˆ‘å¯ä»¥å¸®ä½ åˆ›å»ºä»»åŠ¡ï¼
                """.trimIndent()
            }
            lowerPrompt.contains("ä¹ æƒ¯") || lowerPrompt.contains("åšæŒ") -> {
                """
                å…»æˆå¥½ä¹ æƒ¯éœ€è¦æ—¶é—´å’ŒåšæŒï¼
                å°å»ºè®®ï¼š
                â€¢ ä»å°ç›®æ ‡å¼€å§‹
                â€¢ æ¯å¤©å›ºå®šæ—¶é—´åš
                â€¢ è®°å½•ä½ çš„è¿›åº¦
                â€¢ ç»™è‡ªå·±å¥–åŠ±
                
                æˆ‘å¯ä»¥å¸®ä½ åˆ›å»ºæ¯æ—¥ä»»åŠ¡æ¥è¿½è¸ªä¹ æƒ¯å…»æˆï¼
                """.trimIndent()
            }
            lowerPrompt.contains("è°¢è°¢") || lowerPrompt.contains("æ„Ÿè°¢") -> {
                "ä¸å®¢æ°”ï¼å¾ˆé«˜å…´èƒ½å¸®åˆ°ä½ ï¼\nå¦‚æœè¿˜éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼"
            }
            lowerPrompt.contains("ä½ å¥½") || lowerPrompt.contains("hi") || lowerPrompt.contains("hello") -> {
                "ä½ å¥½ï¼æˆ‘æ˜¯ LifeQuest AI åŠ©æ‰‹\næˆ‘å¯ä»¥å¸®ä½ ç®¡ç†ä»»åŠ¡ã€åˆ¶å®šè®¡åˆ’ã€‚å‘Šè¯‰æˆ‘ä½ æƒ³åšä»€ä¹ˆå§ï¼"
            }
            else -> {
                "æ”¶åˆ°ï¼æˆ‘ä¼šå¸®ä½ å¤„ç†çš„ã€‚\nå¦‚æœéœ€è¦åˆ›å»ºä»»åŠ¡ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“å†…å®¹ï¼"
            }
        }
    }

    /**
     * æ£€æŸ¥æ˜¯å¦å‡†å¤‡å°±ç»ª
     */
    fun isReady(): Boolean = isInitialized

    /**
     * æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–
     */
    fun isModelReady(): Boolean = isInitialized

    /**
     * è·å–æ¨¡å‹è·¯å¾„
     */
    fun getModelPath(): String? = modelPath

    /**
     * æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
     */
    fun isMockMode(): Boolean = useMockMode

    /**
     * é‡Šæ”¾èµ„æº
     */
    fun release() {
        try {
            Log.d(TAG, "Releasing model resources...")

            // âœ… ä½¿ç”¨ LlamaInference é‡Šæ”¾
            llamaInference?.destroy()
            llamaInference = null

            isInitialized = false
            modelPath = null
            Log.d(TAG, "Model resources released")
        } catch (e: Exception) {
            Log.e(TAG, "Error releasing model", e)
        }
    }

    /**
     * è·å–é»˜è®¤æ¨¡å‹è·¯å¾„
     */
    private fun getDefaultModelPath(): String? {
        val modelFile = File(context.filesDir, "ai_models/model.gguf")
        return if (modelFile.exists()) modelFile.absolutePath else null
    }
}